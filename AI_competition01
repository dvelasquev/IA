{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":269359,"sourceType":"datasetVersion","datasetId":111880},{"sourceId":224843881,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":73.034129,"end_time":"2025-02-12T16:08:13.533651","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-02-12T16:07:00.499522","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        break","metadata":{"papermill":{"duration":44.581645,"end_time":"2025-02-12T16:07:49.023612","exception":false,"start_time":"2025-02-12T16:07:04.441967","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T19:05:36.940836Z","iopub.execute_input":"2025-02-27T19:05:36.941335Z","iopub.status.idle":"2025-02-27T19:05:53.190438Z","shell.execute_reply.started":"2025-02-27T19:05:36.941302Z","shell.execute_reply":"2025-02-27T19:05:53.189506Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Description\n\nIn this competition, participants will work with four datasets: training, validation, test, and comp_test. The training dataset contains labeled data that competitors will use to train their models. The validation dataset allows them to fine-tune their models and assess performance during development. The test dataset provides an additional benchmark to estimate how well their models generalize to unseen data. However, the comp_test dataset is different—it consists of unlabeled data for which participants must generate predictions. Their final task is to submit predictions for this dataset, which will be evaluated against hidden ground-truth labels to determine the competition rankings.","metadata":{}},{"cell_type":"code","source":"#Import deep learning libraries\n\nimport torch\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader, random_split, Subset, Dataset\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport numpy as np\nfrom PIL import Image\nimport time\nfrom tqdm import tqdm\n","metadata":{"execution":{"iopub.status.busy":"2025-02-27T19:09:46.81512Z","iopub.execute_input":"2025-02-27T19:09:46.815448Z","iopub.status.idle":"2025-02-27T19:09:46.85314Z","shell.execute_reply.started":"2025-02-27T19:09:46.81542Z","shell.execute_reply":"2025-02-27T19:09:46.852342Z"},"papermill":{"duration":2.41988,"end_time":"2025-02-12T16:07:58.62654","exception":false,"start_time":"2025-02-12T16:07:56.20666","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Step 1: Competition Data Curation and Preprocessing** ","metadata":{}},{"cell_type":"markdown","source":"## Image Preprocessing Pipeline\n\nThe transformation pipeline is designed to preprocess images before feeding them into a machine learning model. The steps involved are:\n\n1. **Convert to Grayscale**:  \n   `transforms.Grayscale(num_output_channels=1)` ensures all images have a single channel, simplifying processing for models that do not require color information.\n\n2. **Resize to a Fixed Dimension**:  \n   `transforms.Resize((150, 150))` resizes all images to a standard size of **150x150 pixels**, ensuring consistency across the dataset.\n\n3. **Convert to Tensor**:  \n   `transforms.ToTensor()` converts images into **PyTorch tensors**, normalizing pixel values to the `[0,1]` range.\n\n4. **Normalize Pixel Values**:  \n   `transforms.Normalize(mean=[0.5], std=[0.5])` standardizes pixel values to have a **zero mean** and a range approximately from `[-1,1]`. This helps stabilize training and improve model performance.\n\nThis preprocessing ensures uniformity across the dataset and enhances the model’s ability to learn meaningful features.\n","metadata":{}},{"cell_type":"code","source":"# Define the transformations\ntransform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n    transforms.Resize((150, 150)),                # Resize images to 150x150\n    transforms.ToTensor(),                        # Convert images to tensors\n    transforms.Normalize(mean=[0.5], std=[0.5])   # Normalize images to [0, 1]\n])","metadata":{"execution":{"iopub.status.busy":"2025-02-27T19:06:44.246645Z","iopub.execute_input":"2025-02-27T19:06:44.247085Z","iopub.status.idle":"2025-02-27T19:06:44.284882Z","shell.execute_reply.started":"2025-02-27T19:06:44.247061Z","shell.execute_reply":"2025-02-27T19:06:44.284083Z"},"papermill":{"duration":0.012806,"end_time":"2025-02-12T16:07:58.643809","exception":false,"start_time":"2025-02-12T16:07:58.631003","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Dataset Preparation\n\n1. **Define the Training Dataset Path**:  \n   The dataset is stored in the directory `/kaggle/input/intel-image-classification/seg_train/seg_train`, which contains images organized into subfolders representing different classes.\n\n2. **Load the Dataset Using `ImageFolder`**:  \n   `datasets.ImageFolder(root=train_path, transform=transform)` automatically assigns labels based on subfolder names and applies the predefined transformations.\n\n3. **Define the Batch Size**:  \n   A `batch_size` of `32` is set, meaning the model will process 32 images at a time during training.\n\n4. **Create the DataLoader**:  \n   `DataLoader(train_dataset, batch_size=batch_size, shuffle=True)` creates an iterable that loads data in mini-batches. The `shuffle=True` ensures that the dataset is randomly shuffled at each epoch, improving generalization and preventing overfitting.\n\nThis setup enables efficient data loading, ensuring that images are properly processed and fed into the model during training.\n","metadata":{}},{"cell_type":"code","source":"#train dataset\ntrain_path = '/kaggle/input/intel-image-classification/seg_train/seg_train'\ntrain_dataset = datasets.ImageFolder(root=train_path, transform=transform)\n# Define batch size\nbatch_size = 32\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# Example: Iterate over the first batch\nfor images, labels in train_loader:\n    print(images.shape, labels.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2025-02-27T19:06:49.709014Z","iopub.execute_input":"2025-02-27T19:06:49.709296Z","iopub.status.idle":"2025-02-27T19:06:59.450985Z","shell.execute_reply.started":"2025-02-27T19:06:49.709275Z","shell.execute_reply":"2025-02-27T19:06:59.450187Z"},"papermill":{"duration":0.011325,"end_time":"2025-02-12T16:08:06.579304","exception":false,"start_time":"2025-02-12T16:08:06.567979","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing Sample Images from the Training Dataset\n\nThis block of code retrieves a batch of images from the `train_loader`, denormalizes them, and plots a subset to visually inspect the dataset.\n\n### **Steps:**\n1. **Iterate Over the DataLoader**:  \n   The loop extracts a batch of `images` and their corresponding `labels` from the `train_loader`.\n\n2. **Denormalize the Images**:  \n   Since the dataset was normalized using `mean=[0.5], std=[0.5]`, the images are denormalized using `images = images * 0.5 + 0.5`, bringing pixel values back to their original scale `[0,1]` for correct visualization.\n\n3. **Plot the Images**:  \n   - `plt.figure(figsize=(10, 10))` sets the figure size.\n   - A loop runs 9 times to plot the first 9 images.\n   - `plt.imshow(images[i][0], cmap='gray')` ensures grayscale images are displayed correctly.\n   - `plt.title(f\"Label: {labels[i].item()}\")` assigns a title with the corresponding label.\n   - `plt.axis('off')` removes axis ticks for better visualization.\n\n4. **Show the Images and Stop the Loop**:  \n   The `break` statement ensures that only the first batch of images is displayed.\n\nThis visualization step helps in understanding the dataset distribution and verifying preprocessing transformations before training the model.\n","metadata":{}},{"cell_type":"code","source":"# Check some examples\nfor images, labels in train_loader:\n    # Denormalize the images for plotting\n    images = images * 0.5 + 0.5\n\n    # Plot the images\n    plt.figure(figsize=(10, 10))\n    for i in range(9):  # Plot 9 images\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i][0], cmap='gray')  # images[i][0] for grayscale images\n        plt.title(f\"Label: {labels[i].item()}\")\n        plt.axis('off')\n    plt.show()\n    break","metadata":{"execution":{"iopub.status.busy":"2025-02-27T19:07:05.850354Z","iopub.execute_input":"2025-02-27T19:07:05.850687Z","iopub.status.idle":"2025-02-27T19:07:07.096498Z","shell.execute_reply.started":"2025-02-27T19:07:05.850665Z","shell.execute_reply":"2025-02-27T19:07:07.095571Z"},"papermill":{"duration":1.244655,"end_time":"2025-02-12T16:08:08.238452","exception":false,"start_time":"2025-02-12T16:08:06.993797","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preparing Validation and Test Datasets with Stratified Sampling\n\nIn this section, we load the validation dataset, apply stratified sampling to ensure class balance, and create separate **validation** and **test** datasets.\n\n### **Steps:**\n\n1. **Load the Full Validation Dataset**:  \n   - The dataset is located at `'/kaggle/input/intel-image-classification/seg_test/seg_test'`.\n   - `datasets.ImageFolder(root=val_path, transform=transform)` loads the dataset and applies the same transformations used for training.\n   - `val_dataset_full.targets` contains the class labels, which are converted into a NumPy array for stratified sampling.\n\n2. **Perform Stratified Sampling**:  \n   - `StratifiedShuffleSplit(n_splits=num_iterations, test_size=sample_size, random_state=42)` ensures that each subset maintains the same class distribution as the full dataset.\n   - `num_iterations=2` creates two non-overlapping samples: one for **validation** and one for **testing**.\n   - The loop iterates through the splits and extracts indices for the validation and test subsets.\n\n3. **Assign the Samples to Validation and Test Sets**:  \n   - The first sample (`sample_datasets[0]`) is used as the **validation dataset**.\n   - The second sample (`sample_datasets[1]`) is used as the **test dataset**.\n\n4. **Create DataLoaders**:  \n   - The `DataLoader` is used to efficiently load data in batches.\n   - A batch size of `100` is set.\n   - `shuffle=False` ensures that the order remains consistent, preserving the stratified class distribution.\n\n### **Purpose of This Approach**:\n- Stratified sampling ensures that both validation and test datasets **maintain the original class distribution**, avoiding potential class imbalance issues.\n- The validation dataset helps fine-tune the model, while the test dataset provides a final unbiased evaluation.\n\nThis setup ensures a fair and balanced model assessment before deploying it on unseen competition data.","metadata":{}},{"cell_type":"code","source":"#Validation and test data\nval_path = '/kaggle/input/intel-image-classification/seg_test/seg_test'\nval_dataset_full = datasets.ImageFolder(root=val_path, transform=transform)\n\ntargets = np.array(val_dataset_full.targets)  # Labels for stratification\nsample_size = 100  # Total sample size\n\nnum_iterations = 2  # Number of different samples you want\n# Create stratified split with multiple iterations\nsplitter = StratifiedShuffleSplit(n_splits=num_iterations, test_size=sample_size, random_state=42)\n\nsample_datasets = []\nfor train_idx, sample_idx in splitter.split(np.zeros(len(targets)), targets):\n    sample_datasets.append(Subset(val_dataset_full, sample_idx))\n\nval_dataset = sample_datasets[0]\ntest_dataset = sample_datasets[1]\n\n# Define batch size\nbatch_size = 100\n# Create DataLoaders\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T19:07:16.652603Z","iopub.execute_input":"2025-02-27T19:07:16.652921Z","iopub.status.idle":"2025-02-27T19:07:20.310995Z","shell.execute_reply.started":"2025-02-27T19:07:16.652898Z","shell.execute_reply":"2025-02-27T19:07:20.310256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check some examples of the validation data\nfor images, labels in val_loader:\n    # Denormalize the images for plotting\n    images = images * 0.5 + 0.5\n\n    # Plot the images\n    plt.figure(figsize=(10, 10))\n    for i in range(9):  # Plot 9 images\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i][0], cmap='gray')  # images[i][0] for grayscale images\n        plt.title(f\"Label: {labels[i].item()}\")\n        plt.axis('off')\n    plt.show()\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T19:07:24.691425Z","iopub.execute_input":"2025-02-27T19:07:24.691752Z","iopub.status.idle":"2025-02-27T19:07:26.279977Z","shell.execute_reply.started":"2025-02-27T19:07:24.691726Z","shell.execute_reply":"2025-02-27T19:07:26.279022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check some examples of the test data\nfor images, labels in test_loader:\n    # Denormalize the images for plotting\n    images = images * 0.5 + 0.5\n\n    # Plot the images\n    plt.figure(figsize=(10, 10))\n    for i in range(9):  # Plot 9 images\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i][0], cmap='gray')  # images[i][0] for grayscale images\n        plt.title(f\"Label: {labels[i].item()}\")\n        plt.axis('off')\n    plt.show()\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T19:07:33.032687Z","iopub.execute_input":"2025-02-27T19:07:33.03302Z","iopub.status.idle":"2025-02-27T19:07:34.762897Z","shell.execute_reply.started":"2025-02-27T19:07:33.032994Z","shell.execute_reply":"2025-02-27T19:07:34.761836Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preparing the Competition Test Dataset\n\nThis section creates a dataset for the **competition test data** (`comp_test`), which consists of **unlabeled images**. The goal is to prepare these images for inference so that participants can generate predictions.\n\n### **Steps:**\n\n1. **List Image Files in the Directory**:  \n   - The directory `'/kaggle/input/intel-image-classification/seg_pred/seg_pred'` contains the competition test images.\n   - `os.listdir(image_directory)` retrieves all filenames from the directory.\n   - The list is **filtered** to include only valid image file formats (`.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`).\n\n2. **Select a Subset of Images**:  \n   - The first `200` images are selected for inference using `image_files[:200]`.  \n   - This ensures a manageable dataset size for predictions.\n\n3. **Define a Custom Dataset (`SelectedFilesDataset`)**:  \n   - A PyTorch `Dataset` is created to handle only the selected images.\n   - It takes the **root directory** and **list of filenames** as inputs.\n   - In `__getitem__`, it loads each image using `PIL.Image.open()` and converts it to **RGB** format.\n   - If transformations (`transform`) are provided, they are applied before returning the image and its filename.\n\n4. **Create the DataLoader**:  \n   - `DataLoader(comp_test_dataset, batch_size=100, shuffle=False)` efficiently loads i\n\n","metadata":{"papermill":{"duration":0.013652,"end_time":"2025-02-12T16:08:08.265381","exception":false,"start_time":"2025-02-12T16:08:08.251729","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Step 1: List the images in the directory\nimage_directory = '/kaggle/input/intel-image-classification/seg_pred/seg_pred'\nimage_files = os.listdir(image_directory)\n\n# Filter out only files with image extensions if necessary\nimage_files = [f for f in image_files if f.endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n\n# Step 2: Select the first 200 images\nimage_files = image_files[:200]\n\nclass SelectedFilesDataset(Dataset):\n    def __init__(self, root_dir, file_names, transform=None):\n        \"\"\"\n        Args:\n            root_dir (str): Directory where files are stored.\n            file_names (list): List of selected filenames (not full paths).\n            transform (callable, optional): Transformations to apply to images.\n        \"\"\"\n        self.root_dir = root_dir\n        self.file_names = file_names\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.file_names)\n\n    def __getitem__(self, idx):\n        file_path = os.path.join(self.root_dir, self.file_names[idx])\n        image = Image.open(file_path).convert(\"RGB\")  # Load image\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, self.file_names[idx]  # Return image + filename\n\n# Example usage\n\ncomp_test_dataset = SelectedFilesDataset(image_directory, image_files, transform = transform)\ncomp_test_loader = DataLoader(comp_test_dataset, batch_size=100, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2025-02-27T19:08:15.09536Z","iopub.execute_input":"2025-02-27T19:08:15.095713Z","iopub.status.idle":"2025-02-27T19:08:15.140938Z","shell.execute_reply.started":"2025-02-27T19:08:15.095687Z","shell.execute_reply":"2025-02-27T19:08:15.140011Z"},"papermill":{"duration":0.058898,"end_time":"2025-02-12T16:08:08.337791","exception":false,"start_time":"2025-02-12T16:08:08.278893","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check some examples of the competition test data\nfor images, labels in comp_test_loader:\n    # Denormalize the images for plotting\n    images = images * 0.5 + 0.5\n\n    # Plot the images\n    plt.figure(figsize=(10, 10))\n    for i in range(9):  # Plot 9 images\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i][0], cmap='gray')  # images[i][0] for grayscale images\n        #plt.title(f\"Label: {labels[i].item()}\")\n        plt.axis('off')\n    plt.show()\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T19:08:18.479291Z","iopub.execute_input":"2025-02-27T19:08:18.479663Z","iopub.status.idle":"2025-02-27T19:08:19.85489Z","shell.execute_reply.started":"2025-02-27T19:08:18.479632Z","shell.execute_reply":"2025-02-27T19:08:19.853975Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 2: Model Training Setup**\n\nThis step focuses on defining the **deep learning model**, setting up the **loss function**, **optimizer**, and establishing the **training loop**.","metadata":{}},{"cell_type":"markdown","source":"### **Define the Model Architecture**\nA simple feed forward deep learning model is created for image classification. The architecture should be appropriate for the dataset, considering its complexity and number of classes.\n","metadata":{}},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T18:46:03.125319Z","iopub.execute_input":"2025-02-27T18:46:03.12562Z","iopub.status.idle":"2025-02-27T18:46:03.147259Z","shell.execute_reply.started":"2025-02-27T18:46:03.125592Z","shell.execute_reply":"2025-02-27T18:46:03.146609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nPATH = '/kaggle/usr/lib/myminitorchv2/'\nsys.path.insert(1, PATH)\nimport myminotorchv2 as mytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T19:08:28.835609Z","iopub.execute_input":"2025-02-27T19:08:28.835903Z","iopub.status.idle":"2025-02-27T19:08:28.877079Z","shell.execute_reply.started":"2025-02-27T19:08:28.835881Z","shell.execute_reply":"2025-02-27T19:08:28.876378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if CUDA is available and set the device accordingly\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice = 'cpu'\n\n# Define the number of input features and output classes\nn_features = 22500\nn_classes = 6\n\n# Initialize the network and loss function\nnet = mytorch.Linear(n_features, n_classes, device=device)\n# Modify the net structure","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T19:08:32.274846Z","iopub.execute_input":"2025-02-27T19:08:32.275124Z","iopub.status.idle":"2025-02-27T19:08:32.413157Z","shell.execute_reply.started":"2025-02-27T19:08:32.275105Z","shell.execute_reply":"2025-02-27T19:08:32.412273Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **2. Define the Loss Function and Optimizer**\n- The **loss function** (e.g., `CrossEntropyLoss`) measures how well the model's predictions match the true labels.\n- The **optimizer** (e.g.,`SGD`) updates the model’s parameters to minimize the loss.","metadata":{}},{"cell_type":"code","source":"CELoss = mytorch.CrossEntropyFromLogits()\n# Set the learning rate\nlearning_rate = 0.001","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T19:08:37.144398Z","iopub.execute_input":"2025-02-27T19:08:37.144772Z","iopub.status.idle":"2025-02-27T19:08:37.181799Z","shell.execute_reply.started":"2025-02-27T19:08:37.144745Z","shell.execute_reply":"2025-02-27T19:08:37.18105Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **3. Training Loop**\nThe model is trained iteratively using:\n1. **Forward pass** – The model makes predictions.\n2. **Loss computation** – The difference between predictions and actual labels is calculated.\n3. **Backward pass** – Gradients are computed via backpropagation.\n4. **Parameter update** – The optimizer updates model weights to minimize loss.\n5. **Evaluation on the validation set** – The model’s performance is monitored to avoid overfitting.","metadata":{}},{"cell_type":"code","source":"# Number of epochs\nnum_epochs = 10\n\n# Lists to store loss values\ntrain_loss_history = []\nval_loss_history = []\n\n# Training loop over epochs\nfor epoch in range(1, num_epochs + 1):\n    print(f\"\\nEpoch {epoch}/{num_epochs}\")\n    \n    # Reset accuracy and loss accumulators\n    total_correct = 0\n    total_samples = 0\n    epoch_loss = 0\n\n    # Training phase\n    for batch_idx, (images, labels) in enumerate(tqdm(train_loader, desc=\"Training Batches\"), 1):\n        # Flatten the images and move them to the device\n        X = images.view(images.shape[0], -1).to(device)\n        Y = labels.to(device)\n\n        # Forward pass: Compute predicted logits\n        Z = net.forward(X)\n\n        # Compute the cross-entropy loss\n        loss = CELoss.forward(Z, Y)\n        epoch_loss += loss.item()\n\n        # Backward pass: Compute gradients\n        dZ = CELoss.backward(n_classes)\n        dX = net.backward(dZ)\n\n        # Update the network parameters\n        net.update(learning_rate)\n\n        # Compute batch accuracy\n        _, predicted = torch.max(Z, 1)\n        correct = (predicted == Y).sum().item()\n        total_correct += correct\n        total_samples += Y.size(0)\n\n        # Print loss and accuracy every 10% of total batches\n        if batch_idx % (len(train_loader) // 10) == 0:\n            tqdm.write(f\"Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}, Acc: {total_correct / total_samples:.4f}\")\n\n    # Compute average loss and accuracy for the epoch\n    train_loss = epoch_loss / len(train_loader)\n    train_accuracy = total_correct / total_samples\n    train_loss_history.append(train_loss)\n\n    print(f\"Epoch {epoch} - Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n\n    # Validation Phase\n    val_correct = 0\n    val_samples = 0\n    val_loss = 0\n\n    with torch.no_grad():  # Disable gradient calculation for validation\n        for images, labels in val_loader:\n            X = images.view(images.shape[0], -1).to(device)\n            Y = labels.to(device)\n\n            # Forward pass\n            Z = net.forward(X)\n\n            # Compute loss\n            loss = CELoss.forward(Z, Y)\n            val_loss += loss.item()\n\n            # Compute accuracy\n            _, predicted = torch.max(Z, 1)\n            val_correct += (predicted == Y).sum().item()\n            val_samples += Y.size(0)\n\n    # Compute validation loss and accuracy\n    val_loss /= len(val_loader)\n    val_accuracy = val_correct / val_samples\n    val_loss_history.append(val_loss)\n\n    print(f\"Epoch {epoch} - Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T19:13:18.795644Z","iopub.execute_input":"2025-02-27T19:13:18.795962Z","iopub.status.idle":"2025-02-27T19:19:09.374715Z","shell.execute_reply.started":"2025-02-27T19:13:18.795936Z","shell.execute_reply":"2025-02-27T19:19:09.373769Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **4. Model Evaluation on Test Set**\n\nAfter training, the model's performance is assessed on the test dataset to evaluate its generalization to unseen data. This involves computing the loss and accuracy without updating model parameters, ensuring an unbiased estimate of its predictive capabilities.\n","metadata":{}},{"cell_type":"code","source":"# Validation Phase\ntest_correct = 0\ntest_samples = 0\ntest_loss = 0\ntest_loss_history = []\n\nwith torch.no_grad():  # Disable gradient calculation for validation\n    for images, labels in test_loader:\n        X = images.view(images.shape[0], -1).to(device)\n        Y = labels.to(device)\n\n        # Forward pass\n        Z = net.forward(X)\n\n        # Compute loss\n        loss = CELoss.forward(Z, Y)\n        test_loss += loss.item()\n\n        # Compute accuracy\n        _, predicted = torch.max(Z, 1)\n        test_correct += (predicted == Y).sum().item()\n        test_samples += Y.size(0)\n\n# Compute validation loss and accuracy\ntest_loss /= len(test_loader)\ntest_accuracy = test_correct / test_samples\ntest_loss_history.append(test_loss)\nprint(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T19:29:18.881933Z","iopub.execute_input":"2025-02-27T19:29:18.882243Z","iopub.status.idle":"2025-02-27T19:29:19.1756Z","shell.execute_reply.started":"2025-02-27T19:29:18.882215Z","shell.execute_reply":"2025-02-27T19:29:19.174664Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **5. Generating Predictions for the Competition Test Set**\n\nAfter training and validating the model, the next step is to generate predictions for the comp_test dataset, which contains unlabeled data. This involves processing the dataset through the trained model to obtain predicted labels. These predictions are then formatted according to the competition's submission requirements, a CSV file with each image's filename and its corresponding predicted label. This submission file is then uploaded to the competition platform for evaluation against the hidden ground-truth labels.\n","metadata":{}},{"cell_type":"code","source":"preds = []\nids = []\nwith torch.no_grad():  # Disable gradient calculation for validation\n    for images,names in comp_test_loader:\n        X = images.view(images.shape[0], -1).to(device)\n\n        # Forward pass\n        Z = net.forward(X)\n        _, predicted = torch.max(Z, 1)\n        preds.append(list(predicted.detach().cpu().numpy()))\n        ids.append(list(names))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T19:59:13.18729Z","iopub.execute_input":"2025-02-27T19:59:13.187699Z","iopub.status.idle":"2025-02-27T19:59:13.945081Z","shell.execute_reply.started":"2025-02-27T19:59:13.187667Z","shell.execute_reply":"2025-02-27T19:59:13.944136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\nids = sum(ids,[])\npreds = sum(preds,[])\n\n\n# Ensure file_names and pred have the same length\nif len(ids) != len(preds):\n    raise ValueError(\"The length of file_names and pred must be the same.\")\n\n# Create a DataFrame from the file_names and pred lists\ndf = pd.DataFrame({\n    'id': ids,\n    'pred': preds\n})\n\n# Define the CSV file name\ncsv_file = 'predictions.csv'\n\n# Save the DataFrame to a CSV file\ndf.to_csv(csv_file, index=False)\n\nprint(f\"CSV file '{csv_file}' created successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T20:00:29.456855Z","iopub.execute_input":"2025-02-27T20:00:29.457162Z","iopub.status.idle":"2025-02-27T20:00:29.507642Z","shell.execute_reply.started":"2025-02-27T20:00:29.45714Z","shell.execute_reply":"2025-02-27T20:00:29.50689Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The file can be downloaded from the folder /kaggle/working","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}